<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.8.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="RHANQTL">
<meta property="og:url" content="http://yoursite.com/page/5/index.html">
<meta property="og:site_name" content="RHANQTL">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RHANQTL">

<link rel="canonical" href="http://yoursite.com/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false
  };
</script>

  <title>RHANQTL</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">RHANQTL</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">上下求索</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/17/python-pandas/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Han Qi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RHANQTL">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/17/python-pandas/" class="post-title-link" itemprop="url">Pandas | 如何判断一个值是否在 DataFrame 的指定列中？</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-17 19:34:14" itemprop="dateCreated datePublished" datetime="2020-04-17T19:34:14+08:00">2020-04-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-18 17:13:11" itemprop="dateModified" datetime="2020-04-18T17:13:11+08:00">2020-04-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><code>in</code> of a Series checks whether the value is in the index:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">11</span>]: s = pd.Series(list(<span class="string">'abc'</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: s</span><br><span class="line">Out[<span class="number">12</span>]: </span><br><span class="line"><span class="number">0</span>    a</span><br><span class="line"><span class="number">1</span>    b</span><br><span class="line"><span class="number">2</span>    c</span><br><span class="line">dtype: object</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: <span class="number">1</span> <span class="keyword">in</span> s</span><br><span class="line">Out[<span class="number">13</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: <span class="string">'a'</span> <span class="keyword">in</span> s</span><br><span class="line">Out[<span class="number">14</span>]: <span class="keyword">False</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>@LeiHuang a Series is a key-value datastructure (I guess so is a DataFrame which acts the same here), also the <code>in</code> can be <em>very</em> efficient on an Index, it’s <code>O(1)</code>, whereas on an array it’ll be <code>O(n)</code>. But you’re right this is something that you do find oneself doing occasionally… my claim is generally there’s a better way/alternative…</p>
</blockquote>
<p>One option is to see if it’s in <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html" target="_blank" rel="noopener">unique</a> values:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">21</span>]: s.unique()</span><br><span class="line">Out[<span class="number">21</span>]: array([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>], dtype=object)</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: <span class="string">'a'</span> <span class="keyword">in</span> s.unique()</span><br><span class="line">Out[<span class="number">22</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<p>or a python set:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">23</span>]: set(s)</span><br><span class="line">Out[<span class="number">23</span>]: &#123;<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: <span class="string">'a'</span> <span class="keyword">in</span> set(s)</span><br><span class="line">Out[<span class="number">24</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<p>As pointed out by @DSM, it may be more efficient (especially if you’re just doing this for one value) to just use in directly on the values:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">31</span>]: s.values</span><br><span class="line">Out[<span class="number">31</span>]: array([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>], dtype=object)</span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: <span class="string">'a'</span> <span class="keyword">in</span> s.values</span><br><span class="line">Out[<span class="number">32</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: s = pd.Series(list(<span class="string">'abc'</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: s</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line"><span class="number">0</span>    a</span><br><span class="line"><span class="number">1</span>    b</span><br><span class="line"><span class="number">2</span>    c</span><br><span class="line">dtype: object</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: s.isin([<span class="string">'a'</span>])</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line"><span class="number">0</span>    <span class="keyword">True</span></span><br><span class="line"><span class="number">1</span>    <span class="keyword">False</span></span><br><span class="line"><span class="number">2</span>    <span class="keyword">False</span></span><br><span class="line">dtype: bool</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: s[s.isin([<span class="string">'a'</span>])].empty</span><br><span class="line">Out[<span class="number">4</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: s[s.isin([<span class="string">'z'</span>])].empty</span><br><span class="line">Out[<span class="number">5</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<p>But this approach can be more flexible if you need to match multiple values at once for a DataFrame (see <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html" target="_blank" rel="noopener">DataFrame.isin</a>)</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = DataFrame(&#123;<span class="string">'A'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">'B'</span>: [<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.isin(&#123;<span class="string">'A'</span>: [<span class="number">1</span>, <span class="number">3</span>], <span class="string">'B'</span>: [<span class="number">4</span>, <span class="number">7</span>, <span class="number">12</span>]&#125;)</span><br><span class="line">       A      B</span><br><span class="line"><span class="number">0</span>   <span class="keyword">True</span>  <span class="keyword">False</span>  <span class="comment"># Note that B didn't match 1 here.</span></span><br><span class="line"><span class="number">1</span>  <span class="keyword">False</span>   <span class="keyword">True</span></span><br><span class="line"><span class="number">2</span>   <span class="keyword">True</span>   <span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<p>conditionally update </p>
<p><a href="https://gist.github.com/pierdom/cab3e44641442cf2c69dc1ead59df0ff" target="_blank" rel="noopener">https://gist.github.com/pierdom/cab3e44641442cf2c69dc1ead59df0ff</a></p>
<p>JOIN</p>
<p><a href="https://blog.csdn.net/zutsoft/article/details/51498026" target="_blank" rel="noopener">https://blog.csdn.net/zutsoft/article/details/51498026</a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://stackoverflow.com/questions/21319929/how-to-determine-whether-a-pandas-column-contains-a-particular-value" target="_blank" rel="noopener">https://stackoverflow.com/questions/21319929/how-to-determine-whether-a-pandas-column-contains-a-particular-value</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/17/nlp-tools-language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Han Qi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RHANQTL">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/17/nlp-tools-language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" class="post-title-link" itemprop="url">【译】使用 torchtext 进行语言建模（实用 torchtext 第二部分）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-17 14:43:48 / 修改时间：23:58:20" itemprop="dateCreated datePublished" datetime="2020-04-17T14:43:48+08:00">2020-04-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>讲解如何使用 torchtext 训练语言模型以及在训练实际的模型时可能会使用到的 torchtext 的一些更实用的功能</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/04/17/nlp-tools-language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/15/nlp-tools-a-comprehensive-tutorial-to-torchtext-practical-torchtext-part-1/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Han Qi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RHANQTL">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/15/nlp-tools-a-comprehensive-tutorial-to-torchtext-practical-torchtext-part-1/" class="post-title-link" itemprop="url">【译】torchtext 概览（实用 torchtext 第一部分）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-15 16:28:08" itemprop="dateCreated datePublished" datetime="2020-04-15T16:28:08+08:00">2020-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-06 08:33:38" itemprop="dateModified" datetime="2020-06-06T08:33:38+08:00">2020-06-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/" target="_blank" rel="noopener">原文地址</a></p>
<h1 id="2-nbsp-nbsp-nbsp-nbsp-声明字段"><a href="#2-nbsp-nbsp-nbsp-nbsp-声明字段" class="headerlink" title="2&nbsp;&nbsp;&nbsp;&nbsp;声明字段"></a>2&nbsp;&nbsp;&nbsp;&nbsp;声明字段</h1><p>torchtext 采用声明性的方式加载其数据：你只要告诉 torchtext 你希望数据看起来如何，然后 torchtext 就会为你处理数据。</p>
<p>实现这件事的方法是声明一个 <code>Field</code>。<code>Field</code> 指定您希望如何处理某个字段。让我们看一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field</span><br><span class="line"></span><br><span class="line">tokenize = <span class="keyword">lambda</span> x: x.split()</span><br><span class="line">TEXT = Field(sequential=<span class="keyword">True</span>, tokenize=tokenize, lower=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">LABEL = Field(sequential=<span class="keyword">False</span>, use_vocab=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>在不良评论分类数据集中，有两种字段：评论文本和标签（toxic, severe toxic, obscene, threat, insult, identity hate）。</p>
<p>让我们先来看一下比较简单的 <code>LABEL</code> 字段。默认情况下，所有字段都希望输入一个单词序列，并且期望接下来再建立从单词到整数的映射（此映射称为 <code>vocab</code>，稍后我们将了解如何创建它）。如果您传递的字段默认情况下已经数字化并且不是顺序的，则应传递 <code>use_vocab = False</code> 和 <code>sequence = False</code>。</p>
<p>对于评论文本，我们传入以关键字参数的方式传入我们希望字段进行的预处理。我们给它提供我们希望该字段使用的分词器，告诉它将输入转换为小写，还告诉它输入是顺序的。</p>
<p>除了上述关键字参数之外，<code>Field</code> 类还允许用户指定特殊 token（用于词汇表外单词的 <code>unk_token</code>，用于填充的 <code>pad_token</code>，用于句子结尾的 <code>eos_token</code> 以及用于句子开头的可选的 <code>init_token</code>，选择是将第一维设为批处理还是序列（默认情况下，第一维为序列），然后选择是允许在运行时确定序列长度还是预先确定序列长度。幸运的是，<code>Field</code> 类的<a href="https://github.com/pytorch/text/blob/c839a7934930819be7e240ea972e4d600966afdc/torchtext/data/field.py#L61" target="_blank" rel="noopener">文档</a>编写得相对好，因此，如果需要一些高级预处理，则应参考它们以获取更多信息。</p>
<p><code>Field</code> 类是 torchtext 的核心，正是它使得预处理如此容易。除了标准 <code>Field</code> 类之外，这是当前可用字段的列表（以及用例）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Field</td>
<td>A regular field that defines preprocessing and postprocessing</td>
<td>Non-text fields and text fields where you don’t need to map integers back to words</td>
</tr>
<tr>
<td>ReversibleField</td>
<td>An extension of the field that allows reverse mapping of word ids to words</td>
<td>Text fields if you want to map the integers back to natural language (such as in the case of language modeling)</td>
</tr>
<tr>
<td>NestedField</td>
<td>A field that takes processes non-tokenized text into a set of smaller fields</td>
<td>Char-based models</td>
</tr>
<tr>
<td>LabelField</td>
<td>A regular field with sequential=False and no <code>&lt;unk&gt;</code> token. Newly added on the master branch of the torchtext github repo, not yet available for release.</td>
<td>Label fields in text classification.</td>
</tr>
</tbody>
</table>
</div>
<h1 id="3-nbsp-nbsp-nbsp-nbsp-构造数据集"><a href="#3-nbsp-nbsp-nbsp-nbsp-构造数据集" class="headerlink" title="3&nbsp;&nbsp;&nbsp;&nbsp;构造数据集"></a>3&nbsp;&nbsp;&nbsp;&nbsp;构造数据集</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> TabularDataset</span><br><span class="line"></span><br><span class="line">tv_datafields = [</span><br><span class="line">    <span class="comment"># 我们不需要 id 字段，所以给它的 `field` 传一个 `None`</span></span><br><span class="line">    (<span class="string">"id"</span>, <span class="keyword">None</span>), </span><br><span class="line">    (<span class="string">"comment_text"</span>, TEXT), </span><br><span class="line">    (<span class="string">"toxic"</span>, LABEL), </span><br><span class="line">    (<span class="string">"severe_toxic"</span>, LABEL), </span><br><span class="line">    (<span class="string">"threat"</span>, LABEL), </span><br><span class="line">    (<span class="string">"obscene"</span>, LABEL), </span><br><span class="line">    (<span class="string">"insult"</span>, LABEL), </span><br><span class="line">    (<span class="string">"identity_hate"</span>, LABEL)</span><br><span class="line">]</span><br><span class="line">trn, vld = TabularDataset.splits(</span><br><span class="line">    <span class="comment"># 数据所在的目录</span></span><br><span class="line">    path=<span class="string">"data"</span>,</span><br><span class="line">    train=<span class="string">'train.csv'</span>, </span><br><span class="line">    validation=<span class="string">"valid.csv"</span>,</span><br><span class="line">    format=<span class="string">'csv'</span>,</span><br><span class="line">    <span class="comment"># 如果你的 csv 文件有字段名 (header)，一定要设置此参数以避免字段名被当成数据处理</span></span><br><span class="line">    skip_header=<span class="keyword">True</span>,</span><br><span class="line">    fields=tv_datafields)</span><br><span class="line"></span><br><span class="line">tst_datafields = [</span><br><span class="line">    (<span class="string">"id"</span>, <span class="keyword">None</span>),</span><br><span class="line">    (<span class="string">"comment_text"</span>, TEXT)</span><br><span class="line">]</span><br><span class="line">tst = TabularDataset(</span><br><span class="line">    path=<span class="string">"data/test.csv"</span>,</span><br><span class="line">    format=<span class="string">'csv'</span>,</span><br><span class="line">    skip_header=<span class="keyword">True</span>,</span><br><span class="line">    fields=tst_datafields)</span><br></pre></td></tr></table></figure>
<p>对于 <code>TabularDataset</code>，我们给 <code>fields</code> 参数传递的值为（名称，字段）对的列表。注意，该列表中字段的顺序必须与数据文件中相同。对于我们不使用的列，我们传入一个 <code>field</code> 元素为 <code>None</code> 的元组（如 <code>(&quot;id&quot;, None)</code>）。<sup><a href="#fn_1" id="reffn_1">1</a></sup></p>
<p><code>splits</code> 方法通过应用相同的处理为训练和验证数据创建数据集。它也可以处理测试数据，但是由于测试数据与训练和验证数据的格式不同，因此我们创建了不同的数据集。</p>
<p>数据集通常可以与列表一样对待。探索一下数据集的内部有助于理解这一点。数据集可以像普通列表一样被索引和迭代，因此让我们看一下第一个元素是什么样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; trn[0]</span><br><span class="line">torchtext.data.example.Example at 0x10d3ed3c8</span><br><span class="line">&gt;&gt;&gt; trn[0].__dict__.keys()</span><br><span class="line">dict_keys([&apos;comment_text&apos;, &apos;toxic&apos;, &apos;severe_toxic&apos;, &apos;threat&apos;, &apos;obscene&apos;, &apos;insult&apos;, &apos;identity_hate&apos;])</span><br><span class="line">&gt;&gt;&gt; trn[0].comment_text[:3]</span><br><span class="line">[&apos;explanation&apos;, &apos;why&apos;, &apos;the&apos;]</span><br></pre></td></tr></table></figure>
<p>我们得到一个 <code>Example</code> 对象。<code>Example</code> 对象将单个数据点的属性放在一起。我们还看到文本已经经过了分词，但尚未转换为整数。这是有道理的，因为我们还没有构造从单词到 ID 的映射。我们的下一步就是构造此映射。</p>
<p>torchtext 处理将单词映射为整数，但是必须告知它应处理的所有单词。在我们的例子中，我们可能只想在训练集上构建词汇表，因此我们运行以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TEXT.build_vocab(trn)</span><br></pre></td></tr></table></figure>
<p>这使 torchtext 遍历训练集中的所有元素，检查与 <code>TEXT</code> 字段相对应的内容，并将单词注册在其词汇表中。torchtext 有用于处理词汇表的 <code>Vocab</code> 类。<code>Vocab</code> 类在其 <code>stoi</code> 属性中包含从单词到 ID 的映射，在其 <code>itos</code> 属性中包含反向映射。除此之外，它还可以使用各种预训练的词嵌入（例如 word2vec）为你自动构建一个嵌入矩阵（要了解详细信息，请参见<a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">另一篇教程</a>）。<code>Vocab</code> 类还可以使用诸如 <code>max_size</code> 和 <code>min_freq</code> 之类的选项，这些选项指示词汇表中的单词至少应该出现多少次。未包含在词汇将被转换成代表“未知”的 <code>&lt;UNK&gt;</code>。</p>
<p>以下是当前可用的数据集及其接收的数据格式的列表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>TabularDataset</td>
<td>Takes paths to csv/tsv files and json files or Python dictionaries as inputs.</td>
<td>Any problem that involves a label (or labels) for each piece of text</td>
</tr>
<tr>
<td>LanguageModelingDataset</td>
<td>Takes the path to a text file as input.</td>
<td>Language modeling</td>
</tr>
<tr>
<td>TranslationDataset</td>
<td>Takes a path and extensions to a file for each language. e.g. If the files are English: “hoge.en”, French: “hoge.fr”, path=”hoge”, exts=(“en”,”fr”)</td>
<td>Translation</td>
</tr>
<tr>
<td>SequenceTaggingDataset</td>
<td>Takes a path to a file with the input sequence and output sequence separated by tabs. <sup><a href="#fn_2" id="reffn_2">2</a></sup></td>
<td>Sequence tagging</td>
</tr>
</tbody>
</table>
</div>
<p>完成数据的格式化并将其读入内存之后，我们转到下一步：创建一个 <code>Iterator</code> 将数据传递给我们的模型。</p>
<h1 id="4-nbsp-nbsp-nbsp-nbsp-构造迭代器"><a href="#4-nbsp-nbsp-nbsp-nbsp-构造迭代器" class="headerlink" title="4&nbsp;&nbsp;&nbsp;&nbsp;构造迭代器"></a>4&nbsp;&nbsp;&nbsp;&nbsp;构造迭代器</h1><p>在 torchvision 和 PyTorch 中，数据的处理和批处理由 <code>DataLoader</code> 处理。由于某种原因，torchtext 将执行完全相同操作的对象重命名为 <code>Iterator</code>。基本功能是相同的，但是正如我们将看到的，<code>Iterator</code> 具有一些 NLP 特有的便捷功能。 以下是有关如何初始化训练、验证和测试数据的迭代器的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Iterator, BucketIterator</span><br><span class="line"></span><br><span class="line">train_iter, val_iter = BucketIterator.splits(</span><br><span class="line">    <span class="comment"># 指定迭代器从何处获取数据</span></span><br><span class="line">    (trn, vld), </span><br><span class="line">    batch_sizes=(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">    <span class="comment"># 如果要使用 GPU，在这里指定 GPU 编号</span></span><br><span class="line">    device=<span class="number">-1</span>,</span><br><span class="line">    <span class="comment"># 需要告诉 `BucketIterator` 用于分组数据的函数</span></span><br><span class="line">    sort_key=<span class="keyword">lambda</span> x: len(x.comment_text),</span><br><span class="line">    sort_within_batch=<span class="keyword">False</span>,</span><br><span class="line">    <span class="comment"># we want to wrap this Iterator layer.</span></span><br><span class="line">    repeat=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">test_iter = Iterator(tst, </span><br><span class="line">                     batch_size=<span class="number">64</span>, </span><br><span class="line">                     device=<span class="number">-1</span>, </span><br><span class="line">                     sort=<span class="keyword">False</span>, </span><br><span class="line">                     sort_within_batch=<span class="keyword">False</span>, </span><br><span class="line">                     repeat=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>更新：<code>sort_within_batch</code> 参数设置为 <code>True</code> 时，将根据 <code>sort_key</code> 以降序对每个 mini-batch 中的数据进行排序。当你要对填充序列数据使用 <code>pack_padded_sequence</code> 并将填充序列张量转换为 <code>PackedSequence</code> 对象时，这是必需的。</p>
<p><code>BucketIterator</code> 是 torchtext 最强大的特性之一。它会自动将输入序列打乱并存储到类似长度的序列中。</p>
<p>正如我前面提到的那样，<code>BucketIterator</code> 强大的原因是，我们需要填充输入序列使得它们的长度相同，这样才能实现批处理。例如，序列</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">3</span>, <span class="number">15</span>, <span class="number">2</span>, <span class="number">7</span>],</span><br><span class="line"> [<span class="number">4</span>, <span class="number">1</span>],</span><br><span class="line"> [<span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<p>需要填充成</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[3, 15, 2, 7, 0],</span><br><span class="line"> [4,  1, 0, 0, 0],</span><br><span class="line"> [5,  5, 6, 8, 1]]</span><br></pre></td></tr></table></figure>
<p>如你所见，所需的填充量由 mini-batch 中的最长序列决定。因此，当序列长度相似时，填充是最有效的。<code>BucketIterator</code> 在后台执行所有这些操作。提醒您，您需要告诉 <code>BucketIterator</code> 您想将数据存储在哪个属性上。在我们的例子中，我们要基于 <code>comment_text</code> 字段的长度进行存储，因此我们将其作为关键字参数传入。有关其他参数的详细信息，请参见上面的代码。</p>
<p>对于测试数据，我们不希望对数据进行打乱，因为我们将在训练结束时输出预测。这就是为什么我们使用标准迭代器。</p>
<p>以下是 torchtext 当前实现的迭代器的列表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Iterator</td>
<td>Iterates over the data in the order of the dataset.</td>
<td>Test data, or any other data where the order is important.</td>
</tr>
<tr>
<td>BucketIterator</td>
<td>Buckets sequences of similar lengths together.</td>
<td>Text classification, sequence tagging, etc. (use cases where the input is of variable length)</td>
</tr>
<tr>
<td>BPTTIterator</td>
<td>专为语言建模而构建的迭代器，它还会生成延迟一个时间步的输入序列。它还会改变 BPTT（时间反向传播）的长度。关于它的详细信息，参见<a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">这篇文章</a></td>
<td>Language modeling</td>
</tr>
</tbody>
</table>
</div>
<h1 id="5-nbsp-nbsp-nbsp-nbsp-包装迭代器"><a href="#5-nbsp-nbsp-nbsp-nbsp-包装迭代器" class="headerlink" title="5&nbsp;&nbsp;&nbsp;&nbsp;包装迭代器"></a>5&nbsp;&nbsp;&nbsp;&nbsp;包装迭代器</h1><p>当前，迭代器返回一个称为 <code>torchtext.data.Batch</code> 的自定义数据类型。<code>Batch</code> 类具有与 <code>Example</code> 类型类似的 API，其中来自每个字段的一批数据作为属性。不幸的是，这种自定义数据类型使代码难以重用（因为每次列名更改时，我们都需要修改代码），并且在某些用例（例如 torchsample 和 fastai）中，torchtext 难以与其他库一起使用。 </p>
<p>我希望以后可以解决此问题（如果我可以决定 API 的外观，我正在考虑提交 PR），但与此同时，我们将使用一个简单的包装程序来简化 <code>Batch</code> 的使用。 </p>
<p>具体来说，我们将 batch 转换为 <code>(x, y)</code> 形式的元组，其中 x 是自变量（模型的输入），而 y 是因变量（监管数据）。这是代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchWrapper</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, x_var, y_vars)</span>:</span></span><br><span class="line">        <span class="comment"># we pass in the list of attributes for x</span></span><br><span class="line">        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</span><br><span class="line">            <span class="comment"># we assume only one input in this wrapper</span></span><br><span class="line">            x = getattr(batch, self.x_var)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># we will concatenate y into a single tensor</span></span><br><span class="line">            <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                y = torch.cat([getattr(batch, feat).unsqueeze(<span class="number">1</span>) </span><br><span class="line">                               <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars], </span><br><span class="line">                              dim=<span class="number">1</span>).float()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y = torch.zeros((<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> (x, y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br><span class="line"></span><br><span class="line">train_dl = BatchWrapper(train_iter, </span><br><span class="line">                        <span class="string">"comment_text"</span>, </span><br><span class="line">                        [<span class="string">"toxic"</span>, </span><br><span class="line">                         <span class="string">"severe_toxic"</span>, </span><br><span class="line">                         <span class="string">"obscene"</span>, </span><br><span class="line">                         <span class="string">"threat"</span>, </span><br><span class="line">                         <span class="string">"insult"</span>, </span><br><span class="line">                         <span class="string">"identity_hate"</span>])</span><br><span class="line">valid_dl = BatchWrapper(val_iter, </span><br><span class="line">                        <span class="string">"comment_text"</span>, </span><br><span class="line">                        [<span class="string">"toxic"</span>, </span><br><span class="line">                         <span class="string">"severe_toxic"</span>, </span><br><span class="line">                         <span class="string">"obscene"</span>, </span><br><span class="line">                         <span class="string">"threat"</span>, </span><br><span class="line">                         <span class="string">"insult"</span>, </span><br><span class="line">                         <span class="string">"identity_hate"</span>])</span><br><span class="line">test_dl = BatchWrapper(test_iter, <span class="string">"comment_text"</span>, <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>我们在这里所做的就是将 <code>Batch</code> 对象转换为输入和输出的元组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; next(train_dl.__iter__())</span><br><span class="line">(Variable containing:</span><br><span class="line">   606   354   334  ...     63    15    15</span><br><span class="line">   693    63    55  ...      4   601    29</span><br><span class="line">   584     4   520  ...    664   242    21</span><br><span class="line">        ...          ⋱          ...</span><br><span class="line">     1     1     1  ...      1     1    84</span><br><span class="line">     1     1     1  ...      1     1   118</span><br><span class="line">     1     1     1  ...      1     1    15</span><br><span class="line"> [torch.LongTensor of size 494x25], Variable containing:</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     1     1     0     1     1     0</span><br><span class="line">     1     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     1     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line"> [torch.FloatTensor of size 25x6])</span><br></pre></td></tr></table></figure>
<p>这里没什么特别的。现在，我们终于准备好开始训练文本分类器了。</p>
<h1 id="6-nbsp-nbsp-nbsp-nbsp-训练模型"><a href="#6-nbsp-nbsp-nbsp-nbsp-训练模型" class="headerlink" title="6&nbsp;&nbsp;&nbsp;&nbsp;训练模型"></a>6&nbsp;&nbsp;&nbsp;&nbsp;训练模型</h1><p>我们将使用一个简单的 LSTM 来演示如何在已构建的数据上训练文本分类器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLSTMBaseline</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_dim, emb_dim=<span class="number">300</span>, num_linear=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(SimpleLSTMBaseline, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)</span><br><span class="line">        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=<span class="number">1</span>)</span><br><span class="line">        self.linear_layers = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_linear - <span class="number">1</span>):</span><br><span class="line">            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))</span><br><span class="line">            self.linear_layers = nn.ModuleList(self.linear_layers)</span><br><span class="line">        self.predictor = nn.Linear(hidden_dim, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, seq)</span>:</span></span><br><span class="line">        hdn, _ = self.encoder(self.embedding(seq))</span><br><span class="line">        feature = hdn[<span class="number">-1</span>, :, :]</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.linear_layers:</span><br><span class="line">          feature = layer(feature)</span><br><span class="line">          preds = self.predictor(feature)</span><br><span class="line">        <span class="keyword">return</span> preds</span><br><span class="line"></span><br><span class="line">em_sz = <span class="number">100</span></span><br><span class="line">nh = <span class="number">500</span></span><br><span class="line">nl = <span class="number">3</span></span><br><span class="line">model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz)</span><br></pre></td></tr></table></figure>
<p>现在，我们将编写训练循环。感谢我们所有的预处理，这非常简单。我们可以使用包装的 <code>Iterator</code> 进行迭代，然后将数据移至 GPU 并进行适当的数字化处理后，数据会自动传递给我们。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">opt = optim.Adam(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">loss_func = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    running_corrects = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> tqdm.tqdm(train_dl):</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        preds = model(x)</span><br><span class="line">        loss = loss_func(y, preds)</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.data[<span class="number">0</span>] * x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    epoch_loss = running_loss / len(trn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate the validation loss for this epoch</span></span><br><span class="line">    val_loss = <span class="number">0.0</span></span><br><span class="line">    model.eval() <span class="comment"># turn on evaluation mode</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> valid_dl:</span><br><span class="line">        preds = model(x)</span><br><span class="line">        loss = loss_func(y, preds)</span><br><span class="line">        val_loss += loss.data[<span class="number">0</span>] * x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    val_loss /= len(vld)</span><br><span class="line">    print(<span class="string">'Epoch: &#123;&#125;, '</span></span><br><span class="line">          <span class="string">'Training Loss: &#123;:.4f&#125;, '</span></span><br><span class="line">          <span class="string">'Validation Loss: &#123;:.4f&#125;'</span>.format(epoch, epoch_loss, val_loss))</span><br></pre></td></tr></table></figure>
<p>这里没有什么要解释的：这只是一个标准的训练循环。现在，让我们生成我们的预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test_preds = []</span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> tqdm.tqdm(test_dl):</span><br><span class="line">    preds = model(x)</span><br><span class="line">    preds = preds.data.numpy()</span><br><span class="line">    <span class="comment"># the actual outputs of the model are logits, so we need to pass these values to the sigmoid function</span></span><br><span class="line">    preds = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-preds))</span><br><span class="line">    test_preds.append(preds)</span><br><span class="line">    test_preds = np.hstack(test_preds)</span><br></pre></td></tr></table></figure>
<p>最后，我们可以将预测写入 csv 文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">"data/test.csv"</span>)</span><br><span class="line"><span class="keyword">for</span> i, col <span class="keyword">in</span> enumerate([<span class="string">"toxic"</span>, <span class="string">"severe_toxic"</span>, <span class="string">"obscene"</span>, <span class="string">"threat"</span>, <span class="string">"insult"</span>, <span class="string">"identity_hate"</span>]):</span><br><span class="line">    df[col] = test_preds[:, i]</span><br><span class="line"></span><br><span class="line">df.drop(<span class="string">"comment_text"</span>, axis=<span class="number">1</span>).to_csv(<span class="string">"submission.csv"</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>我们完成了！我们可以将该文件提交给 Kaggle，尝试完善我们的模型，更改分词器或其他类似方法，并且只需要对上面的代码进行一些更改即可。</p>
<h1 id="7-nbsp-nbsp-nbsp-nbsp-结论和进一步阅读"><a href="#7-nbsp-nbsp-nbsp-nbsp-结论和进一步阅读" class="headerlink" title="7&nbsp;&nbsp;&nbsp;&nbsp;结论和进一步阅读"></a>7&nbsp;&nbsp;&nbsp;&nbsp;结论和进一步阅读</h1><p>我希望本教程能够深入了解 torchtext 的使用方式以及其用途。尽管该库仍然很新，并且存在许多粗糙的地方，但我相信 torchtext 是朝着标准化文本预处理迈出的重要第一步，它将提高全世界 NLP 工作人员的工作效率。 如果您想查看 torchtext 用于语言建模的信息，我已上传了<a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">另一个教程</a>，其中详细介绍了语言建模和 BPTT 迭代器。如果您还有其他问题，请随时在评论中问我！</p>
<blockquote id="fn_1">
<sup>1</sup>. 下一个版本的 torchtext（以及 GitHub 上的当前版本）将能够使用字典，将名称按列映射到其对应的字段而不是列表。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. 该 API 是用于 tsv 格式的 <code>TabularDataset</code> API 的子集，因此将来可能会被废弃。<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/15/nlp-tools-a-comprehensive-tutorial-to-torchtext/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Han Qi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RHANQTL">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/15/nlp-tools-a-comprehensive-tutorial-to-torchtext/" class="post-title-link" itemprop="url">【译】torchtext 概览（实用 torchtext 第一部分）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-15 16:28:08" itemprop="dateCreated datePublished" datetime="2020-04-15T16:28:08+08:00">2020-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-18 17:12:47" itemprop="dateModified" datetime="2020-04-18T17:12:47+08:00">2020-04-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/" target="_blank" rel="noopener">原文地址</a></p>
<h1 id="2-nbsp-nbsp-nbsp-nbsp-声明字段"><a href="#2-nbsp-nbsp-nbsp-nbsp-声明字段" class="headerlink" title="2&nbsp;&nbsp;&nbsp;&nbsp;声明字段"></a>2&nbsp;&nbsp;&nbsp;&nbsp;声明字段</h1><p>torchtext 采用声明性的方式加载其数据：你只要告诉 torchtext 你希望数据看起来如何，然后 torchtext 就会为你处理数据。</p>
<p>实现这件事的方法是声明一个 <code>Field</code>。<code>Field</code> 指定您希望如何处理某个字段。让我们看一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field</span><br><span class="line"></span><br><span class="line">tokenize = <span class="keyword">lambda</span> x: x.split()</span><br><span class="line">TEXT = Field(sequential=<span class="keyword">True</span>, tokenize=tokenize, lower=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">LABEL = Field(sequential=<span class="keyword">False</span>, use_vocab=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>在不良评论分类数据集中，有两种字段：评论文本和标签（toxic, severe toxic, obscene, threat, insult, identity hate）。</p>
<p>让我们先来看一下比较简单的 <code>LABEL</code> 字段。默认情况下，所有字段都希望输入一个单词序列，并且期望接下来再建立从单词到整数的映射（此映射称为 <code>vocab</code>，稍后我们将了解如何创建它）。如果您传递的字段默认情况下已经数字化并且不是顺序的，则应传递 <code>use_vocab = False</code> 和 <code>sequence = False</code>。</p>
<p>对于评论文本，我们传入以关键字参数的方式传入我们希望字段进行的预处理。我们给它提供我们希望该字段使用的分词器，告诉它将输入转换为小写，还告诉它输入是顺序的。</p>
<p>除了上述关键字参数之外，<code>Field</code> 类还允许用户指定特殊 token（用于词汇表外单词的 <code>unk_token</code>，用于填充的 <code>pad_token</code>，用于句子结尾的 <code>eos_token</code> 以及用于句子开头的可选的 <code>init_token</code>，选择是将第一维设为批处理还是序列（默认情况下，第一维为序列），然后选择是允许在运行时确定序列长度还是预先确定序列长度。幸运的是，<code>Field</code> 类的<a href="https://github.com/pytorch/text/blob/c839a7934930819be7e240ea972e4d600966afdc/torchtext/data/field.py#L61" target="_blank" rel="noopener">文档</a>编写得相对好，因此，如果需要一些高级预处理，则应参考它们以获取更多信息。</p>
<p><code>Field</code> 类是 torchtext 的核心，正是它使得预处理如此容易。除了标准 <code>Field</code> 类之外，这是当前可用字段的列表（以及用例）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Field</td>
<td>A regular field that defines preprocessing and postprocessing</td>
<td>Non-text fields and text fields where you don’t need to map integers back to words</td>
</tr>
<tr>
<td>ReversibleField</td>
<td>An extension of the field that allows reverse mapping of word ids to words</td>
<td>Text fields if you want to map the integers back to natural language (such as in the case of language modeling)</td>
</tr>
<tr>
<td>NestedField</td>
<td>A field that takes processes non-tokenized text into a set of smaller fields</td>
<td>Char-based models</td>
</tr>
<tr>
<td>LabelField (New!)</td>
<td>A regular field with sequential=False and no <unk> token. Newly added on the master branch of the torchtext github repo, not yet available for release.</unk></td>
<td>Label fields in text classification.</td>
</tr>
</tbody>
</table>
</div>
<h1 id="3-nbsp-nbsp-nbsp-nbsp-构造数据集"><a href="#3-nbsp-nbsp-nbsp-nbsp-构造数据集" class="headerlink" title="3&nbsp;&nbsp;&nbsp;&nbsp;构造数据集"></a>3&nbsp;&nbsp;&nbsp;&nbsp;构造数据集</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> TabularDataset</span><br><span class="line"></span><br><span class="line">tv_datafields = [</span><br><span class="line">    <span class="comment"># 我们不需要 id 字段，所以给它的 `field` 传一个 `None`</span></span><br><span class="line">    (<span class="string">"id"</span>, <span class="keyword">None</span>), </span><br><span class="line">    (<span class="string">"comment_text"</span>, TEXT), </span><br><span class="line">    (<span class="string">"toxic"</span>, LABEL), </span><br><span class="line">    (<span class="string">"severe_toxic"</span>, LABEL), </span><br><span class="line">    (<span class="string">"threat"</span>, LABEL), </span><br><span class="line">    (<span class="string">"obscene"</span>, LABEL), </span><br><span class="line">    (<span class="string">"insult"</span>, LABEL), </span><br><span class="line">    (<span class="string">"identity_hate"</span>, LABEL)</span><br><span class="line">]</span><br><span class="line">trn, vld = TabularDataset.splits(</span><br><span class="line">    <span class="comment"># 数据所在的目录</span></span><br><span class="line">    path=<span class="string">"data"</span>,</span><br><span class="line">    train=<span class="string">'train.csv'</span>, </span><br><span class="line">    validation=<span class="string">"valid.csv"</span>,</span><br><span class="line">    format=<span class="string">'csv'</span>,</span><br><span class="line">    <span class="comment"># 如果你的 csv 文件有字段名 (header)，一定要设置此参数以避免字段名被当成数据处理</span></span><br><span class="line">    skip_header=<span class="keyword">True</span>,</span><br><span class="line">    fields=tv_datafields)</span><br><span class="line"></span><br><span class="line">tst_datafields = [</span><br><span class="line">    (<span class="string">"id"</span>, <span class="keyword">None</span>),</span><br><span class="line">    (<span class="string">"comment_text"</span>, TEXT)</span><br><span class="line">]</span><br><span class="line">tst = TabularDataset(</span><br><span class="line">    path=<span class="string">"data/test.csv"</span>,</span><br><span class="line">    format=<span class="string">'csv'</span>,</span><br><span class="line">    skip_header=<span class="keyword">True</span>,</span><br><span class="line">    fields=tst_datafields)</span><br></pre></td></tr></table></figure>
<p>对于 <code>TabularDataset</code>，我们给 <code>fields</code> 参数传递的值为（名称，字段）对的列表。注意，该列表中字段的顺序必须与数据文件中相同。对于我们不使用的列，我们传入一个 <code>field</code> 元素为 <code>None</code> 的元组（如 <code>(&quot;id&quot;, None)</code>）。<sup><a href="#fn_1" id="reffn_1">1</a></sup></p>
<p><code>splits</code> 方法通过应用相同的处理为训练和验证数据创建数据集。它也可以处理测试数据，但是由于测试数据与训练和验证数据的格式不同，因此我们创建了不同的数据集。</p>
<p>数据集通常可以与列表一样对待。探索一下数据集的内部有助于理解这一点。数据集可以像普通列表一样被索引和迭代，因此让我们看一下第一个元素是什么样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; trn[0]</span><br><span class="line">torchtext.data.example.Example at 0x10d3ed3c8</span><br><span class="line">&gt;&gt;&gt; trn[0].__dict__.keys()</span><br><span class="line">dict_keys([&apos;comment_text&apos;, &apos;toxic&apos;, &apos;severe_toxic&apos;, &apos;threat&apos;, &apos;obscene&apos;, &apos;insult&apos;, &apos;identity_hate&apos;])</span><br><span class="line">&gt;&gt;&gt; trn[0].comment_text[:3]</span><br><span class="line">[&apos;explanation&apos;, &apos;why&apos;, &apos;the&apos;]</span><br></pre></td></tr></table></figure>
<p>我们得到一个 <code>Example</code> 对象。<code>Example</code> 对象将单个数据点的属性放在一起。我们还看到文本已经经过了分词，但尚未转换为整数。这是有道理的，因为我们还没有构造从单词到 ID 的映射。我们的下一步就是构造此映射。</p>
<p>torchtext 处理将单词映射为整数，但是必须告知它应处理的所有单词。在我们的例子中，我们可能只想在训练集上构建词汇表，因此我们运行以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TEXT.build_vocab(trn)</span><br></pre></td></tr></table></figure>
<p>这使 torchtext 遍历训练集中的所有元素，检查与 <code>TEXT</code> 字段相对应的内容，并将单词注册在其词汇表中。torchtext 有用于处理词汇表的 <code>Vocab</code> 类。<code>Vocab</code> 类在其 <code>stoi</code> 属性中包含从单词到 ID 的映射，在其 <code>itos</code> 属性中包含反向映射。除此之外，它还可以使用各种预训练的词嵌入（例如 word2vec）为你自动构建一个嵌入矩阵（要了解详细信息，请参见<a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">另一篇教程</a>）。<code>Vocab</code> 类还可以使用诸如 <code>max_size</code> 和 <code>min_freq</code> 之类的选项，这些选项指示词汇表中的单词至少应该出现多少次。未包含在词汇将被转换成代表“未知”的 <code>&lt;UNK&gt;</code>。</p>
<p>以下是当前可用的数据集及其接收的数据格式的列表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>TabularDataset</td>
<td>Takes paths to csv/tsv files and json files or Python dictionaries as inputs.</td>
<td>Any problem that involves a label (or labels) for each piece of text</td>
</tr>
<tr>
<td>LanguageModelingDataset</td>
<td>Takes the path to a text file as input.</td>
<td>Language modeling</td>
</tr>
<tr>
<td>TranslationDataset</td>
<td>Takes a path and extensions to a file for each language. e.g. If the files are English: “hoge.en”, French: “hoge.fr”, path=”hoge”, exts=(“en”,”fr”)</td>
<td>Translation</td>
</tr>
<tr>
<td>SequenceTaggingDataset</td>
<td>Takes a path to a file with the input sequence and output sequence separated by tabs. <sup><a href="#fn_2" id="reffn_2">2</a></sup></td>
<td>Sequence tagging</td>
</tr>
</tbody>
</table>
</div>
<p>完成数据的格式化并将其读入内存之后，我们转到下一步：创建一个 <code>Iterator</code> 将数据传递给我们的模型。</p>
<h1 id="4-nbsp-nbsp-nbsp-nbsp-构造迭代器"><a href="#4-nbsp-nbsp-nbsp-nbsp-构造迭代器" class="headerlink" title="4&nbsp;&nbsp;&nbsp;&nbsp;构造迭代器"></a>4&nbsp;&nbsp;&nbsp;&nbsp;构造迭代器</h1><p>在 torchvision 和 PyTorch 中，数据的处理和批处理由 <code>DataLoader</code> 处理。由于某种原因，torchtext 将执行完全相同操作的对象重命名为 <code>Iterator</code>。基本功能是相同的，但是正如我们将看到的，<code>Iterator</code> 具有一些 NLP 特有的便捷功能。 以下是有关如何初始化训练、验证和测试数据的迭代器的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Iterator, BucketIterator</span><br><span class="line"></span><br><span class="line">train_iter, val_iter = BucketIterator.splits(</span><br><span class="line">    <span class="comment"># 指定迭代器从何处获取数据</span></span><br><span class="line">    (trn, vld), </span><br><span class="line">    batch_sizes=(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">    <span class="comment"># 如果要使用 GPU，在这里指定 GPU 编号</span></span><br><span class="line">    device=<span class="number">-1</span>,</span><br><span class="line">    <span class="comment"># 需要告诉 `BucketIterator` 用于分组数据的函数</span></span><br><span class="line">    sort_key=<span class="keyword">lambda</span> x: len(x.comment_text),</span><br><span class="line">    sort_within_batch=<span class="keyword">False</span>,</span><br><span class="line">    <span class="comment"># we want to wrap this Iterator layer.</span></span><br><span class="line">    repeat=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">test_iter = Iterator(tst, </span><br><span class="line">                     batch_size=<span class="number">64</span>, </span><br><span class="line">                     device=<span class="number">-1</span>, </span><br><span class="line">                     sort=<span class="keyword">False</span>, </span><br><span class="line">                     sort_within_batch=<span class="keyword">False</span>, </span><br><span class="line">                     repeat=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>更新：<code>sort_within_batch</code> 参数设置为 <code>True</code> 时，将根据 <code>sort_key</code> 以降序对每个 mini-batch 中的数据进行排序。当你要对填充序列数据使用 <code>pack_padded_sequence</code> 并将填充序列张量转换为 <code>PackedSequence</code> 对象时，这是必需的。</p>
<p><code>BucketIterator</code> 是 torchtext 最强大的特性之一。它会自动将输入序列打乱并存储到类似长度的序列中。</p>
<p>正如我前面提到的那样，<code>BucketIterator</code> 强大的原因是，我们需要填充输入序列使得它们的长度相同，这样才能实现批处理。例如，序列</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">3</span>, <span class="number">15</span>, <span class="number">2</span>, <span class="number">7</span>],</span><br><span class="line"> [<span class="number">4</span>, <span class="number">1</span>],</span><br><span class="line"> [<span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<p>需要填充成</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[3, 15, 2, 7, 0],</span><br><span class="line"> [4,  1, 0, 0, 0],</span><br><span class="line"> [5,  5, 6, 8, 1]]</span><br></pre></td></tr></table></figure>
<p>如你所见，所需的填充量由 mini-batch 中的最长序列决定。因此，当序列长度相似时，填充是最有效的。<code>BucketIterator</code> 在后台执行所有这些操作。提醒您，您需要告诉 <code>BucketIterator</code> 您想将数据存储在哪个属性上。在我们的例子中，我们要基于 <code>comment_text</code> 字段的长度进行存储，因此我们将其作为关键字参数传入。有关其他参数的详细信息，请参见上面的代码。</p>
<p>对于测试数据，我们不希望对数据进行打乱，因为我们将在训练结束时输出预测。这就是为什么我们使用标准迭代器。</p>
<p>以下是 torchtext 当前实现的迭代器的列表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Iterator</td>
<td>Iterates over the data in the order of the dataset.</td>
<td>Test data, or any other data where the order is important.</td>
</tr>
<tr>
<td>BucketIterator</td>
<td>Buckets sequences of similar lengths together.</td>
<td>Text classification, sequence tagging, etc. (use cases where the input is of variable length)</td>
</tr>
<tr>
<td>BPTTIterator</td>
<td>专为语言建模而构建的迭代器，它还会生成延迟一个时间步的输入序列。它还会改变 BPTT（时间反向传播）的长度。关于它的详细信息，参见<a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">这篇文章</a></td>
<td>Language modeling</td>
</tr>
</tbody>
</table>
</div>
<h1 id="5-nbsp-nbsp-nbsp-nbsp-包装迭代器"><a href="#5-nbsp-nbsp-nbsp-nbsp-包装迭代器" class="headerlink" title="5&nbsp;&nbsp;&nbsp;&nbsp;包装迭代器"></a>5&nbsp;&nbsp;&nbsp;&nbsp;包装迭代器</h1><p>当前，迭代器返回一个称为 <code>torchtext.data.Batch</code> 的自定义数据类型。<code>Batch</code> 类具有与 <code>Example</code> 类型类似的 API，其中来自每个字段的一批数据作为属性。不幸的是，这种自定义数据类型使代码难以重用（因为每次列名更改时，我们都需要修改代码），并且在某些用例（例如 torchsample 和 fastai）中，torchtext 难以与其他库一起使用。 </p>
<p>我希望以后可以解决此问题（如果我可以决定 API 的外观，我正在考虑提交 PR），但与此同时，我们将使用一个简单的包装程序来简化 <code>Batch</code> 的使用。 </p>
<p>具体来说，我们将 batch 转换为 <code>(x, y)</code> 形式的元组，其中 x 是自变量（模型的输入），而 y 是因变量（监管数据）。这是代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchWrapper</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, x_var, y_vars)</span>:</span></span><br><span class="line">        <span class="comment"># we pass in the list of attributes for x</span></span><br><span class="line">        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</span><br><span class="line">            <span class="comment"># we assume only one input in this wrapper</span></span><br><span class="line">            x = getattr(batch, self.x_var)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># we will concatenate y into a single tensor</span></span><br><span class="line">            <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                y = torch.cat([getattr(batch, feat).unsqueeze(<span class="number">1</span>) </span><br><span class="line">                               <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars], </span><br><span class="line">                              dim=<span class="number">1</span>).float()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y = torch.zeros((<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> (x, y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br><span class="line"></span><br><span class="line">train_dl = BatchWrapper(train_iter, </span><br><span class="line">                        <span class="string">"comment_text"</span>, </span><br><span class="line">                        [<span class="string">"toxic"</span>, </span><br><span class="line">                         <span class="string">"severe_toxic"</span>, </span><br><span class="line">                         <span class="string">"obscene"</span>, </span><br><span class="line">                         <span class="string">"threat"</span>, </span><br><span class="line">                         <span class="string">"insult"</span>, </span><br><span class="line">                         <span class="string">"identity_hate"</span>])</span><br><span class="line">valid_dl = BatchWrapper(val_iter, </span><br><span class="line">                        <span class="string">"comment_text"</span>, </span><br><span class="line">                        [<span class="string">"toxic"</span>, </span><br><span class="line">                         <span class="string">"severe_toxic"</span>, </span><br><span class="line">                         <span class="string">"obscene"</span>, </span><br><span class="line">                         <span class="string">"threat"</span>, </span><br><span class="line">                         <span class="string">"insult"</span>, </span><br><span class="line">                         <span class="string">"identity_hate"</span>])</span><br><span class="line">test_dl = BatchWrapper(test_iter, <span class="string">"comment_text"</span>, <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>我们在这里所做的就是将 <code>Batch</code> 对象转换为输入和输出的元组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; next(train_dl.__iter__())</span><br><span class="line">(Variable containing:</span><br><span class="line">   606   354   334  ...     63    15    15</span><br><span class="line">   693    63    55  ...      4   601    29</span><br><span class="line">   584     4   520  ...    664   242    21</span><br><span class="line">        ...          ⋱          ...</span><br><span class="line">     1     1     1  ...      1     1    84</span><br><span class="line">     1     1     1  ...      1     1   118</span><br><span class="line">     1     1     1  ...      1     1    15</span><br><span class="line"> [torch.LongTensor of size 494x25], Variable containing:</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     1     1     0     1     1     0</span><br><span class="line">     1     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     1     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line"> [torch.FloatTensor of size 25x6])</span><br></pre></td></tr></table></figure>
<p>这里没什么特别的。现在，我们终于准备好开始训练文本分类器了。</p>
<h1 id="6-nbsp-nbsp-nbsp-nbsp-训练模型"><a href="#6-nbsp-nbsp-nbsp-nbsp-训练模型" class="headerlink" title="6&nbsp;&nbsp;&nbsp;&nbsp;训练模型"></a>6&nbsp;&nbsp;&nbsp;&nbsp;训练模型</h1><p>我们将使用一个简单的 LSTM 来演示如何在已构建的数据上训练文本分类器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLSTMBaseline</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_dim, emb_dim=<span class="number">300</span>, num_linear=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(SimpleLSTMBaseline, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)</span><br><span class="line">        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=<span class="number">1</span>)</span><br><span class="line">        self.linear_layers = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_linear - <span class="number">1</span>):</span><br><span class="line">            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))</span><br><span class="line">            self.linear_layers = nn.ModuleList(self.linear_layers)</span><br><span class="line">        self.predictor = nn.Linear(hidden_dim, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, seq)</span>:</span></span><br><span class="line">        hdn, _ = self.encoder(self.embedding(seq))</span><br><span class="line">        feature = hdn[<span class="number">-1</span>, :, :]</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.linear_layers:</span><br><span class="line">          feature = layer(feature)</span><br><span class="line">          preds = self.predictor(feature)</span><br><span class="line">        <span class="keyword">return</span> preds</span><br><span class="line"></span><br><span class="line">em_sz = <span class="number">100</span></span><br><span class="line">nh = <span class="number">500</span></span><br><span class="line">nl = <span class="number">3</span></span><br><span class="line">model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz)</span><br></pre></td></tr></table></figure>
<p>现在，我们将编写训练循环。感谢我们所有的预处理，这非常简单。我们可以使用包装的 <code>Iterator</code> 进行迭代，然后将数据移至 GPU 并进行适当的数字化处理后，数据会自动传递给我们。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">opt = optim.Adam(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">loss_func = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    running_corrects = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> tqdm.tqdm(train_dl):</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        preds = model(x)</span><br><span class="line">        loss = loss_func(y, preds)</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.data[<span class="number">0</span>] * x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    epoch_loss = running_loss / len(trn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate the validation loss for this epoch</span></span><br><span class="line">    val_loss = <span class="number">0.0</span></span><br><span class="line">    model.eval() <span class="comment"># turn on evaluation mode</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> valid_dl:</span><br><span class="line">        preds = model(x)</span><br><span class="line">        loss = loss_func(y, preds)</span><br><span class="line">        val_loss += loss.data[<span class="number">0</span>] * x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    val_loss /= len(vld)</span><br><span class="line">    print(<span class="string">'Epoch: &#123;&#125;, '</span></span><br><span class="line">          <span class="string">'Training Loss: &#123;:.4f&#125;, '</span></span><br><span class="line">          <span class="string">'Validation Loss: &#123;:.4f&#125;'</span>.format(epoch, epoch_loss, val_loss))</span><br></pre></td></tr></table></figure>
<p>这里没有什么要解释的：这只是一个标准的训练循环。现在，让我们生成我们的预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test_preds = []</span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> tqdm.tqdm(test_dl):</span><br><span class="line">    preds = model(x)</span><br><span class="line">    preds = preds.data.numpy()</span><br><span class="line">    <span class="comment"># the actual outputs of the model are logits, so we need to pass these values to the sigmoid function</span></span><br><span class="line">    preds = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-preds))</span><br><span class="line">    test_preds.append(preds)</span><br><span class="line">    test_preds = np.hstack(test_preds)</span><br></pre></td></tr></table></figure>
<p>最后，我们可以将预测写入 csv 文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">"data/test.csv"</span>)</span><br><span class="line"><span class="keyword">for</span> i, col <span class="keyword">in</span> enumerate([<span class="string">"toxic"</span>, <span class="string">"severe_toxic"</span>, <span class="string">"obscene"</span>, <span class="string">"threat"</span>, <span class="string">"insult"</span>, <span class="string">"identity_hate"</span>]):</span><br><span class="line">    df[col] = test_preds[:, i]</span><br><span class="line"></span><br><span class="line">df.drop(<span class="string">"comment_text"</span>, axis=<span class="number">1</span>).to_csv(<span class="string">"submission.csv"</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>我们完成了！我们可以将该文件提交给 Kaggle，尝试完善我们的模型，更改分词器或其他类似方法，并且只需要对上面的代码进行一些更改即可。</p>
<h1 id="7-nbsp-nbsp-nbsp-nbsp-结论和进一步阅读"><a href="#7-nbsp-nbsp-nbsp-nbsp-结论和进一步阅读" class="headerlink" title="7&nbsp;&nbsp;&nbsp;&nbsp;结论和进一步阅读"></a>7&nbsp;&nbsp;&nbsp;&nbsp;结论和进一步阅读</h1><p>我希望本教程能够深入了解 torchtext 的使用方式以及其用途。尽管该库仍然很新，并且存在许多粗糙的地方，但我相信 torchtext 是朝着标准化文本预处理迈出的重要第一步，它将提高全世界 NLP 工作人员的工作效率。 如果您想查看 torchtext 用于语言建模的信息，我已上传了<a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">另一个教程</a>，其中详细介绍了语言建模和 BPTT 迭代器。如果您还有其他问题，请随时在评论中问我！</p>
<blockquote id="fn_1">
<sup>1</sup>. 下一个版本的 torchtext（以及 GitHub 上的当前版本）将能够使用字典，将名称按列映射到其对应的字段而不是列表。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. 该 API 是用于 tsv 格式的 <code>TabularDataset</code> API 的子集，因此将来可能会被废弃。<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/15/tools-pycharm-upload-files-to-remote-server/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Han Qi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RHANQTL">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/15/tools-pycharm-upload-files-to-remote-server/" class="post-title-link" itemprop="url">PyCharm 中将文件上传到远程服务器</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-15 15:46:27 / 修改时间：15:57:28" itemprop="dateCreated datePublished" datetime="2020-04-15T15:46:27+08:00">2020-04-15</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>注意：</strong>社区版的 PyCharm 不支持远程连接服务器的功能，需要使用专业版</p>
<p>首先配置 PyCharm 服务器的代码同步，打开最上方工具栏处的 <code>Tools -&gt; Deployment -&gt; Configuration</code>，在弹出的窗口处点击左边的 <code>+</code> 添加一个部署配置，在弹出的下拉栏中点击选择 <code>SFTP</code>，输入名字然后确认。</p>
<p><img src="/2020/04/15/tools-pycharm-upload-files-to-remote-server/Users\rhanqtl\AppData\Roaming\Typora\typora-user-images\image-20200415155244923.png" alt="image-20200415155244923"></p>
<ol>
<li>配置 SSH Configuration、Root path（根目录或者用户主目录）和 Web server URL（服务器 Host）</li>
<li>配置 Mappings，Deployment path 是远程服务器上项目的根目录，Web path 跟 Deployment Path 一样即可。</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/13/others-yaml-vs-json/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Han Qi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RHANQTL">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/13/others-yaml-vs-json/" class="post-title-link" itemprop="url">others-yaml-vs-json</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-13 15:15:07 / 修改时间：18:58:01" itemprop="dateCreated datePublished" datetime="2020-04-13T15:15:07+08:00">2020-04-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="https://www.json2yaml.com/yaml-vs-json" target="_blank" rel="noopener">https://www.json2yaml.com/yaml-vs-json</a></p>
<p>总结一下：YAML 的优势在于作为配置文件（更强大的语法，允许注释），而 JSON 的优势在于作为数据交换的格式（语法简单，易读）</p>
<p>比如 hexo 的文章就是用 YAML 配置的</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/10/nlp-tools-and-datasets/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Han Qi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RHANQTL">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/10/nlp-tools-and-datasets/" class="post-title-link" itemprop="url">NLP 中的工具和数据集</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-10 20:23:00" itemprop="dateCreated datePublished" datetime="2020-04-10T20:23:00+08:00">2020-04-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-15 21:34:51" itemprop="dateModified" datetime="2020-04-15T21:34:51+08:00">2020-04-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="https://aclweb.org/aclwiki/" target="_blank" rel="noopener">https://aclweb.org/aclwiki/</a></p>
<h1 id="1-nbsp-nbsp-nbsp-nbsp-数据集"><a href="#1-nbsp-nbsp-nbsp-nbsp-数据集" class="headerlink" title="1&nbsp;&nbsp;&nbsp;&nbsp;数据集"></a>1&nbsp;&nbsp;&nbsp;&nbsp;数据集</h1><p><strong>WordNet</strong></p>
<p><a href="https://www.nltk.org/howto/wordnet.html" target="_blank" rel="noopener">https://www.nltk.org/howto/wordnet.html</a></p>
<p><a href="https://wordnet.princeton.edu/" target="_blank" rel="noopener">Homepage</a></p>
<p>在 Python 中使用：通过 NLTK</p>
<blockquote>
<p>初次使用时可能需要下载：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; &gt;&gt;&gt; import nltk</span><br><span class="line">&gt; &gt;&gt;&gt; nltk.download(&quot;wordnet&quot;)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from nltk.corpus import wordnet</span><br></pre></td></tr></table></figure>
<p><strong>COW</strong></p>
<p><a href="https://corporafromtheweb.org/encow14/" target="_blank" rel="noopener">https://corporafromtheweb.org/encow14/</a></p>
<h1 id="2-nbsp-nbsp-nbsp-nbsp-工具"><a href="#2-nbsp-nbsp-nbsp-nbsp-工具" class="headerlink" title="2&nbsp;&nbsp;&nbsp;&nbsp;工具"></a>2&nbsp;&nbsp;&nbsp;&nbsp;工具</h1><p>gensim</p>
<p><strong>word2vec</strong></p>
<p><strong>fastText</strong></p>
<p>BERT</p>
<p>spacy</p>
<p><code>torchtext</code> (requires <code>sentencepiece</code>)</p>
<p><code>torchtext.data.DataSet</code></p>
<p><code>torchtext.data.TabularDataSet</code></p>
<p><code>torchtext.vocab.Vectors()</code></p>
<p>nltk</p>
<p>re (regex)</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] <a href="https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908" target="_blank" rel="noopener">Text Preprocessing in Python: Steps, Tools, and Examples</a>（注：其中移除标点符号的代码在 Python 3.7 中无法使用，解决办法参见 <a href="https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string" target="_blank" rel="noopener">Best way to strip punctuation from a string</a>）</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/05/others-raid/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Han Qi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RHANQTL">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/05/others-raid/" class="post-title-link" itemprop="url">others-raid</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-05 18:58:47 / 修改时间：19:44:17" itemprop="dateCreated datePublished" datetime="2020-04-05T18:58:47+08:00">2020-04-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>从可靠性 (reliability)、可用性 (availability)、性能 (performance) 和容量 (capacity) 四个方面考量。</p>
<h1 id="1-nbsp-nbsp-nbsp-nbsp-标准级别"><a href="#1-nbsp-nbsp-nbsp-nbsp-标准级别" class="headerlink" title="1&nbsp;&nbsp;&nbsp;&nbsp;标准级别"></a>1&nbsp;&nbsp;&nbsp;&nbsp;标准级别</h1><p><strong>RAID 0</strong></p>
<p>将数据分块存放在不同的硬盘上，没有备份和校验</p>
<ul>
<li>可靠性：差，如果硬盘组中的任何一块硬盘损坏，数据就会丢失<ul>
<li>平均失效 (failure) 率比同等的单块硬盘还要差。可以计算一下，假设单块硬盘损坏导致数据丢失的概率是 0.02，那么两块硬盘组成的 RAID 0 因硬盘损坏而导致数据丢失的概率是 2 <em> 0.02 </em> 0.98 + 0.02 * 0.02 = 0.0396</li>
</ul>
</li>
<li>可用性</li>
<li>性能：吞吐量 = 单块硬盘吞吐量 * N（N 为硬盘数量）<ul>
<li>硬盘可以并行读 / 写</li>
</ul>
</li>
<li>容量：100%</li>
</ul>
<p><strong>RAID 1</strong></p>
<p>将相同的数据存放在多块硬盘上，没有分块和校验</p>
<ul>
<li>可靠性：只要有一块硬盘正常运转，就可以取得数据</li>
<li>可用性</li>
<li>性能<ul>
<li>如果控制器 / 软件经过优化，吞吐量能够与 RAID 0 相当（大多数 RAID 1 实现的实际读吞吐量都比最快的硬盘慢，写吞吐量总是比最快的硬盘慢（要写多个硬盘，时间取决于最慢的那一块））</li>
<li>读 / 写请求可以进行广播，并由寻道时间、旋转时间较短的那块硬盘先响应</li>
</ul>
</li>
<li>容量：50%</li>
</ul>
<p><strong>RAID 2</strong></p>
<p>位级别的分块和相应的专用 Hamming 码校验位（已经成为历史）</p>
<p><strong>RAID 3</strong></p>
<p>字节级别的分块和相应的专用校验位</p>
<p><strong>RAID 4</strong></p>
<p>块级别的分块和专用的校验位</p>
<p><strong>RAID 5</strong></p>
<p>块级别的分布和分布式的校验位</p>
<p>不同于 RAID 4，校验信息分布在各个硬盘上</p>
<p>如果单块硬盘出现失效，后续的读能够根据分布式校验位计算出来，因而不会有数据丢失</p>
<p>至少需要三块硬盘</p>
<p><strong>RAID 6</strong></p>
<p>块级别的分布和分布式的双份校验位</p>
<p>至少需要四块硬盘</p>
<h1 id="2-nbsp-nbsp-nbsp-nbsp-嵌套（混合）RAID"><a href="#2-nbsp-nbsp-nbsp-nbsp-嵌套（混合）RAID" class="headerlink" title="2&nbsp;&nbsp;&nbsp;&nbsp;嵌套（混合）RAID"></a>2&nbsp;&nbsp;&nbsp;&nbsp;嵌套（混合）RAID</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] <a href="">RAID</a></p>
<p>[2] <a href="">Disk mirroring</a></p>
<p>[] <a href="">Data striping</a></p>
<p>[3] <a href="">Standard RAID levels</a></p>
<p>[] <a href="">Nested RAID levels</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/05/soft-eng-to-be-explicit/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Han Qi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RHANQTL">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/05/soft-eng-to-be-explicit/" class="post-title-link" itemprop="url">【译】To Be Explicit</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-05 06:39:48" itemprop="dateCreated datePublished" datetime="2020-04-05T06:39:48+08:00">2020-04-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-06 23:10:26" itemprop="dateModified" datetime="2020-04-06T23:10:26+08:00">2020-04-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>关于代码构造的一篇文章，原作者 Martin Fowler</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/04/05/soft-eng-to-be-explicit/">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/03/howto-verify-a-gpg-signature/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Han Qi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RHANQTL">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/03/howto-verify-a-gpg-signature/" class="post-title-link" itemprop="url">howto-verify-a-gpg-signature</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-03 08:27:44 / 修改时间：08:28:21" itemprop="dateCreated datePublished" datetime="2020-04-03T08:27:44+08:00">2020-04-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="https://www.wikihow.com/Verify-a-GPG-Signature" target="_blank" rel="noopener">How to Verify a GPG Signature</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Han Qi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">277</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">76</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Han Qi</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.2
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
