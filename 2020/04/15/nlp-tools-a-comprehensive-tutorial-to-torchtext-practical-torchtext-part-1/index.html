<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.8.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="原文地址 2&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;声明字段torchtext 采用声明性的方式加载其数据：你只要告诉 torchtext 你希望数据看起来如何，然后 torchtext 就会为你处理数据。 实现这件事的方法是声明一个 Field。Field 指定您希望如何处理某个字段。让我们看一个例子： 123456from torchtext.data import Fieldtoke">
<meta name="keywords" content="NLP,数据预处理 (Data Preprocessing),torchtext">
<meta property="og:type" content="article">
<meta property="og:title" content="【译】torchtext 概览（实用 torchtext 第一部分）">
<meta property="og:url" content="http://yoursite.com/2020/04/15/nlp-tools-a-comprehensive-tutorial-to-torchtext-practical-torchtext-part-1/index.html">
<meta property="og:site_name" content="RHANQTL">
<meta property="og:description" content="原文地址 2&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;声明字段torchtext 采用声明性的方式加载其数据：你只要告诉 torchtext 你希望数据看起来如何，然后 torchtext 就会为你处理数据。 实现这件事的方法是声明一个 Field。Field 指定您希望如何处理某个字段。让我们看一个例子： 123456from torchtext.data import Fieldtoke">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-06-06T00:33:38.875Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【译】torchtext 概览（实用 torchtext 第一部分）">
<meta name="twitter:description" content="原文地址 2&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;声明字段torchtext 采用声明性的方式加载其数据：你只要告诉 torchtext 你希望数据看起来如何，然后 torchtext 就会为你处理数据。 实现这件事的方法是声明一个 Field。Field 指定您希望如何处理某个字段。让我们看一个例子： 123456from torchtext.data import Fieldtoke">

<link rel="canonical" href="http://yoursite.com/2020/04/15/nlp-tools-a-comprehensive-tutorial-to-torchtext-practical-torchtext-part-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>【译】torchtext 概览（实用 torchtext 第一部分） | RHANQTL</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">RHANQTL</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">上下求索</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/15/nlp-tools-a-comprehensive-tutorial-to-torchtext-practical-torchtext-part-1/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Han Qi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RHANQTL">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【译】torchtext 概览（实用 torchtext 第一部分）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-15 16:28:08" itemprop="dateCreated datePublished" datetime="2020-04-15T16:28:08+08:00">2020-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-06 08:33:38" itemprop="dateModified" datetime="2020-06-06T08:33:38+08:00">2020-06-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a href="https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/" target="_blank" rel="noopener">原文地址</a></p>
<h1 id="2-nbsp-nbsp-nbsp-nbsp-声明字段"><a href="#2-nbsp-nbsp-nbsp-nbsp-声明字段" class="headerlink" title="2&nbsp;&nbsp;&nbsp;&nbsp;声明字段"></a>2&nbsp;&nbsp;&nbsp;&nbsp;声明字段</h1><p>torchtext 采用声明性的方式加载其数据：你只要告诉 torchtext 你希望数据看起来如何，然后 torchtext 就会为你处理数据。</p>
<p>实现这件事的方法是声明一个 <code>Field</code>。<code>Field</code> 指定您希望如何处理某个字段。让我们看一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field</span><br><span class="line"></span><br><span class="line">tokenize = <span class="keyword">lambda</span> x: x.split()</span><br><span class="line">TEXT = Field(sequential=<span class="keyword">True</span>, tokenize=tokenize, lower=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">LABEL = Field(sequential=<span class="keyword">False</span>, use_vocab=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>在不良评论分类数据集中，有两种字段：评论文本和标签（toxic, severe toxic, obscene, threat, insult, identity hate）。</p>
<p>让我们先来看一下比较简单的 <code>LABEL</code> 字段。默认情况下，所有字段都希望输入一个单词序列，并且期望接下来再建立从单词到整数的映射（此映射称为 <code>vocab</code>，稍后我们将了解如何创建它）。如果您传递的字段默认情况下已经数字化并且不是顺序的，则应传递 <code>use_vocab = False</code> 和 <code>sequence = False</code>。</p>
<p>对于评论文本，我们传入以关键字参数的方式传入我们希望字段进行的预处理。我们给它提供我们希望该字段使用的分词器，告诉它将输入转换为小写，还告诉它输入是顺序的。</p>
<p>除了上述关键字参数之外，<code>Field</code> 类还允许用户指定特殊 token（用于词汇表外单词的 <code>unk_token</code>，用于填充的 <code>pad_token</code>，用于句子结尾的 <code>eos_token</code> 以及用于句子开头的可选的 <code>init_token</code>，选择是将第一维设为批处理还是序列（默认情况下，第一维为序列），然后选择是允许在运行时确定序列长度还是预先确定序列长度。幸运的是，<code>Field</code> 类的<a href="https://github.com/pytorch/text/blob/c839a7934930819be7e240ea972e4d600966afdc/torchtext/data/field.py#L61" target="_blank" rel="noopener">文档</a>编写得相对好，因此，如果需要一些高级预处理，则应参考它们以获取更多信息。</p>
<p><code>Field</code> 类是 torchtext 的核心，正是它使得预处理如此容易。除了标准 <code>Field</code> 类之外，这是当前可用字段的列表（以及用例）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Field</td>
<td>A regular field that defines preprocessing and postprocessing</td>
<td>Non-text fields and text fields where you don’t need to map integers back to words</td>
</tr>
<tr>
<td>ReversibleField</td>
<td>An extension of the field that allows reverse mapping of word ids to words</td>
<td>Text fields if you want to map the integers back to natural language (such as in the case of language modeling)</td>
</tr>
<tr>
<td>NestedField</td>
<td>A field that takes processes non-tokenized text into a set of smaller fields</td>
<td>Char-based models</td>
</tr>
<tr>
<td>LabelField</td>
<td>A regular field with sequential=False and no <code>&lt;unk&gt;</code> token. Newly added on the master branch of the torchtext github repo, not yet available for release.</td>
<td>Label fields in text classification.</td>
</tr>
</tbody>
</table>
</div>
<h1 id="3-nbsp-nbsp-nbsp-nbsp-构造数据集"><a href="#3-nbsp-nbsp-nbsp-nbsp-构造数据集" class="headerlink" title="3&nbsp;&nbsp;&nbsp;&nbsp;构造数据集"></a>3&nbsp;&nbsp;&nbsp;&nbsp;构造数据集</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> TabularDataset</span><br><span class="line"></span><br><span class="line">tv_datafields = [</span><br><span class="line">    <span class="comment"># 我们不需要 id 字段，所以给它的 `field` 传一个 `None`</span></span><br><span class="line">    (<span class="string">"id"</span>, <span class="keyword">None</span>), </span><br><span class="line">    (<span class="string">"comment_text"</span>, TEXT), </span><br><span class="line">    (<span class="string">"toxic"</span>, LABEL), </span><br><span class="line">    (<span class="string">"severe_toxic"</span>, LABEL), </span><br><span class="line">    (<span class="string">"threat"</span>, LABEL), </span><br><span class="line">    (<span class="string">"obscene"</span>, LABEL), </span><br><span class="line">    (<span class="string">"insult"</span>, LABEL), </span><br><span class="line">    (<span class="string">"identity_hate"</span>, LABEL)</span><br><span class="line">]</span><br><span class="line">trn, vld = TabularDataset.splits(</span><br><span class="line">    <span class="comment"># 数据所在的目录</span></span><br><span class="line">    path=<span class="string">"data"</span>,</span><br><span class="line">    train=<span class="string">'train.csv'</span>, </span><br><span class="line">    validation=<span class="string">"valid.csv"</span>,</span><br><span class="line">    format=<span class="string">'csv'</span>,</span><br><span class="line">    <span class="comment"># 如果你的 csv 文件有字段名 (header)，一定要设置此参数以避免字段名被当成数据处理</span></span><br><span class="line">    skip_header=<span class="keyword">True</span>,</span><br><span class="line">    fields=tv_datafields)</span><br><span class="line"></span><br><span class="line">tst_datafields = [</span><br><span class="line">    (<span class="string">"id"</span>, <span class="keyword">None</span>),</span><br><span class="line">    (<span class="string">"comment_text"</span>, TEXT)</span><br><span class="line">]</span><br><span class="line">tst = TabularDataset(</span><br><span class="line">    path=<span class="string">"data/test.csv"</span>,</span><br><span class="line">    format=<span class="string">'csv'</span>,</span><br><span class="line">    skip_header=<span class="keyword">True</span>,</span><br><span class="line">    fields=tst_datafields)</span><br></pre></td></tr></table></figure>
<p>对于 <code>TabularDataset</code>，我们给 <code>fields</code> 参数传递的值为（名称，字段）对的列表。注意，该列表中字段的顺序必须与数据文件中相同。对于我们不使用的列，我们传入一个 <code>field</code> 元素为 <code>None</code> 的元组（如 <code>(&quot;id&quot;, None)</code>）。<sup><a href="#fn_1" id="reffn_1">1</a></sup></p>
<p><code>splits</code> 方法通过应用相同的处理为训练和验证数据创建数据集。它也可以处理测试数据，但是由于测试数据与训练和验证数据的格式不同，因此我们创建了不同的数据集。</p>
<p>数据集通常可以与列表一样对待。探索一下数据集的内部有助于理解这一点。数据集可以像普通列表一样被索引和迭代，因此让我们看一下第一个元素是什么样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; trn[0]</span><br><span class="line">torchtext.data.example.Example at 0x10d3ed3c8</span><br><span class="line">&gt;&gt;&gt; trn[0].__dict__.keys()</span><br><span class="line">dict_keys([&apos;comment_text&apos;, &apos;toxic&apos;, &apos;severe_toxic&apos;, &apos;threat&apos;, &apos;obscene&apos;, &apos;insult&apos;, &apos;identity_hate&apos;])</span><br><span class="line">&gt;&gt;&gt; trn[0].comment_text[:3]</span><br><span class="line">[&apos;explanation&apos;, &apos;why&apos;, &apos;the&apos;]</span><br></pre></td></tr></table></figure>
<p>我们得到一个 <code>Example</code> 对象。<code>Example</code> 对象将单个数据点的属性放在一起。我们还看到文本已经经过了分词，但尚未转换为整数。这是有道理的，因为我们还没有构造从单词到 ID 的映射。我们的下一步就是构造此映射。</p>
<p>torchtext 处理将单词映射为整数，但是必须告知它应处理的所有单词。在我们的例子中，我们可能只想在训练集上构建词汇表，因此我们运行以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TEXT.build_vocab(trn)</span><br></pre></td></tr></table></figure>
<p>这使 torchtext 遍历训练集中的所有元素，检查与 <code>TEXT</code> 字段相对应的内容，并将单词注册在其词汇表中。torchtext 有用于处理词汇表的 <code>Vocab</code> 类。<code>Vocab</code> 类在其 <code>stoi</code> 属性中包含从单词到 ID 的映射，在其 <code>itos</code> 属性中包含反向映射。除此之外，它还可以使用各种预训练的词嵌入（例如 word2vec）为你自动构建一个嵌入矩阵（要了解详细信息，请参见<a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">另一篇教程</a>）。<code>Vocab</code> 类还可以使用诸如 <code>max_size</code> 和 <code>min_freq</code> 之类的选项，这些选项指示词汇表中的单词至少应该出现多少次。未包含在词汇将被转换成代表“未知”的 <code>&lt;UNK&gt;</code>。</p>
<p>以下是当前可用的数据集及其接收的数据格式的列表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>TabularDataset</td>
<td>Takes paths to csv/tsv files and json files or Python dictionaries as inputs.</td>
<td>Any problem that involves a label (or labels) for each piece of text</td>
</tr>
<tr>
<td>LanguageModelingDataset</td>
<td>Takes the path to a text file as input.</td>
<td>Language modeling</td>
</tr>
<tr>
<td>TranslationDataset</td>
<td>Takes a path and extensions to a file for each language. e.g. If the files are English: “hoge.en”, French: “hoge.fr”, path=”hoge”, exts=(“en”,”fr”)</td>
<td>Translation</td>
</tr>
<tr>
<td>SequenceTaggingDataset</td>
<td>Takes a path to a file with the input sequence and output sequence separated by tabs. <sup><a href="#fn_2" id="reffn_2">2</a></sup></td>
<td>Sequence tagging</td>
</tr>
</tbody>
</table>
</div>
<p>完成数据的格式化并将其读入内存之后，我们转到下一步：创建一个 <code>Iterator</code> 将数据传递给我们的模型。</p>
<h1 id="4-nbsp-nbsp-nbsp-nbsp-构造迭代器"><a href="#4-nbsp-nbsp-nbsp-nbsp-构造迭代器" class="headerlink" title="4&nbsp;&nbsp;&nbsp;&nbsp;构造迭代器"></a>4&nbsp;&nbsp;&nbsp;&nbsp;构造迭代器</h1><p>在 torchvision 和 PyTorch 中，数据的处理和批处理由 <code>DataLoader</code> 处理。由于某种原因，torchtext 将执行完全相同操作的对象重命名为 <code>Iterator</code>。基本功能是相同的，但是正如我们将看到的，<code>Iterator</code> 具有一些 NLP 特有的便捷功能。 以下是有关如何初始化训练、验证和测试数据的迭代器的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Iterator, BucketIterator</span><br><span class="line"></span><br><span class="line">train_iter, val_iter = BucketIterator.splits(</span><br><span class="line">    <span class="comment"># 指定迭代器从何处获取数据</span></span><br><span class="line">    (trn, vld), </span><br><span class="line">    batch_sizes=(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">    <span class="comment"># 如果要使用 GPU，在这里指定 GPU 编号</span></span><br><span class="line">    device=<span class="number">-1</span>,</span><br><span class="line">    <span class="comment"># 需要告诉 `BucketIterator` 用于分组数据的函数</span></span><br><span class="line">    sort_key=<span class="keyword">lambda</span> x: len(x.comment_text),</span><br><span class="line">    sort_within_batch=<span class="keyword">False</span>,</span><br><span class="line">    <span class="comment"># we want to wrap this Iterator layer.</span></span><br><span class="line">    repeat=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">test_iter = Iterator(tst, </span><br><span class="line">                     batch_size=<span class="number">64</span>, </span><br><span class="line">                     device=<span class="number">-1</span>, </span><br><span class="line">                     sort=<span class="keyword">False</span>, </span><br><span class="line">                     sort_within_batch=<span class="keyword">False</span>, </span><br><span class="line">                     repeat=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>更新：<code>sort_within_batch</code> 参数设置为 <code>True</code> 时，将根据 <code>sort_key</code> 以降序对每个 mini-batch 中的数据进行排序。当你要对填充序列数据使用 <code>pack_padded_sequence</code> 并将填充序列张量转换为 <code>PackedSequence</code> 对象时，这是必需的。</p>
<p><code>BucketIterator</code> 是 torchtext 最强大的特性之一。它会自动将输入序列打乱并存储到类似长度的序列中。</p>
<p>正如我前面提到的那样，<code>BucketIterator</code> 强大的原因是，我们需要填充输入序列使得它们的长度相同，这样才能实现批处理。例如，序列</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">3</span>, <span class="number">15</span>, <span class="number">2</span>, <span class="number">7</span>],</span><br><span class="line"> [<span class="number">4</span>, <span class="number">1</span>],</span><br><span class="line"> [<span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<p>需要填充成</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[3, 15, 2, 7, 0],</span><br><span class="line"> [4,  1, 0, 0, 0],</span><br><span class="line"> [5,  5, 6, 8, 1]]</span><br></pre></td></tr></table></figure>
<p>如你所见，所需的填充量由 mini-batch 中的最长序列决定。因此，当序列长度相似时，填充是最有效的。<code>BucketIterator</code> 在后台执行所有这些操作。提醒您，您需要告诉 <code>BucketIterator</code> 您想将数据存储在哪个属性上。在我们的例子中，我们要基于 <code>comment_text</code> 字段的长度进行存储，因此我们将其作为关键字参数传入。有关其他参数的详细信息，请参见上面的代码。</p>
<p>对于测试数据，我们不希望对数据进行打乱，因为我们将在训练结束时输出预测。这就是为什么我们使用标准迭代器。</p>
<p>以下是 torchtext 当前实现的迭代器的列表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Iterator</td>
<td>Iterates over the data in the order of the dataset.</td>
<td>Test data, or any other data where the order is important.</td>
</tr>
<tr>
<td>BucketIterator</td>
<td>Buckets sequences of similar lengths together.</td>
<td>Text classification, sequence tagging, etc. (use cases where the input is of variable length)</td>
</tr>
<tr>
<td>BPTTIterator</td>
<td>专为语言建模而构建的迭代器，它还会生成延迟一个时间步的输入序列。它还会改变 BPTT（时间反向传播）的长度。关于它的详细信息，参见<a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">这篇文章</a></td>
<td>Language modeling</td>
</tr>
</tbody>
</table>
</div>
<h1 id="5-nbsp-nbsp-nbsp-nbsp-包装迭代器"><a href="#5-nbsp-nbsp-nbsp-nbsp-包装迭代器" class="headerlink" title="5&nbsp;&nbsp;&nbsp;&nbsp;包装迭代器"></a>5&nbsp;&nbsp;&nbsp;&nbsp;包装迭代器</h1><p>当前，迭代器返回一个称为 <code>torchtext.data.Batch</code> 的自定义数据类型。<code>Batch</code> 类具有与 <code>Example</code> 类型类似的 API，其中来自每个字段的一批数据作为属性。不幸的是，这种自定义数据类型使代码难以重用（因为每次列名更改时，我们都需要修改代码），并且在某些用例（例如 torchsample 和 fastai）中，torchtext 难以与其他库一起使用。 </p>
<p>我希望以后可以解决此问题（如果我可以决定 API 的外观，我正在考虑提交 PR），但与此同时，我们将使用一个简单的包装程序来简化 <code>Batch</code> 的使用。 </p>
<p>具体来说，我们将 batch 转换为 <code>(x, y)</code> 形式的元组，其中 x 是自变量（模型的输入），而 y 是因变量（监管数据）。这是代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchWrapper</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, x_var, y_vars)</span>:</span></span><br><span class="line">        <span class="comment"># we pass in the list of attributes for x</span></span><br><span class="line">        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</span><br><span class="line">            <span class="comment"># we assume only one input in this wrapper</span></span><br><span class="line">            x = getattr(batch, self.x_var)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># we will concatenate y into a single tensor</span></span><br><span class="line">            <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                y = torch.cat([getattr(batch, feat).unsqueeze(<span class="number">1</span>) </span><br><span class="line">                               <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars], </span><br><span class="line">                              dim=<span class="number">1</span>).float()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y = torch.zeros((<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> (x, y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br><span class="line"></span><br><span class="line">train_dl = BatchWrapper(train_iter, </span><br><span class="line">                        <span class="string">"comment_text"</span>, </span><br><span class="line">                        [<span class="string">"toxic"</span>, </span><br><span class="line">                         <span class="string">"severe_toxic"</span>, </span><br><span class="line">                         <span class="string">"obscene"</span>, </span><br><span class="line">                         <span class="string">"threat"</span>, </span><br><span class="line">                         <span class="string">"insult"</span>, </span><br><span class="line">                         <span class="string">"identity_hate"</span>])</span><br><span class="line">valid_dl = BatchWrapper(val_iter, </span><br><span class="line">                        <span class="string">"comment_text"</span>, </span><br><span class="line">                        [<span class="string">"toxic"</span>, </span><br><span class="line">                         <span class="string">"severe_toxic"</span>, </span><br><span class="line">                         <span class="string">"obscene"</span>, </span><br><span class="line">                         <span class="string">"threat"</span>, </span><br><span class="line">                         <span class="string">"insult"</span>, </span><br><span class="line">                         <span class="string">"identity_hate"</span>])</span><br><span class="line">test_dl = BatchWrapper(test_iter, <span class="string">"comment_text"</span>, <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>我们在这里所做的就是将 <code>Batch</code> 对象转换为输入和输出的元组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; next(train_dl.__iter__())</span><br><span class="line">(Variable containing:</span><br><span class="line">   606   354   334  ...     63    15    15</span><br><span class="line">   693    63    55  ...      4   601    29</span><br><span class="line">   584     4   520  ...    664   242    21</span><br><span class="line">        ...          ⋱          ...</span><br><span class="line">     1     1     1  ...      1     1    84</span><br><span class="line">     1     1     1  ...      1     1   118</span><br><span class="line">     1     1     1  ...      1     1    15</span><br><span class="line"> [torch.LongTensor of size 494x25], Variable containing:</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     1     1     0     1     1     0</span><br><span class="line">     1     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     1     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line">     0     0     0     0     0     0</span><br><span class="line"> [torch.FloatTensor of size 25x6])</span><br></pre></td></tr></table></figure>
<p>这里没什么特别的。现在，我们终于准备好开始训练文本分类器了。</p>
<h1 id="6-nbsp-nbsp-nbsp-nbsp-训练模型"><a href="#6-nbsp-nbsp-nbsp-nbsp-训练模型" class="headerlink" title="6&nbsp;&nbsp;&nbsp;&nbsp;训练模型"></a>6&nbsp;&nbsp;&nbsp;&nbsp;训练模型</h1><p>我们将使用一个简单的 LSTM 来演示如何在已构建的数据上训练文本分类器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLSTMBaseline</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_dim, emb_dim=<span class="number">300</span>, num_linear=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(SimpleLSTMBaseline, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)</span><br><span class="line">        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=<span class="number">1</span>)</span><br><span class="line">        self.linear_layers = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_linear - <span class="number">1</span>):</span><br><span class="line">            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))</span><br><span class="line">            self.linear_layers = nn.ModuleList(self.linear_layers)</span><br><span class="line">        self.predictor = nn.Linear(hidden_dim, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, seq)</span>:</span></span><br><span class="line">        hdn, _ = self.encoder(self.embedding(seq))</span><br><span class="line">        feature = hdn[<span class="number">-1</span>, :, :]</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.linear_layers:</span><br><span class="line">          feature = layer(feature)</span><br><span class="line">          preds = self.predictor(feature)</span><br><span class="line">        <span class="keyword">return</span> preds</span><br><span class="line"></span><br><span class="line">em_sz = <span class="number">100</span></span><br><span class="line">nh = <span class="number">500</span></span><br><span class="line">nl = <span class="number">3</span></span><br><span class="line">model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz)</span><br></pre></td></tr></table></figure>
<p>现在，我们将编写训练循环。感谢我们所有的预处理，这非常简单。我们可以使用包装的 <code>Iterator</code> 进行迭代，然后将数据移至 GPU 并进行适当的数字化处理后，数据会自动传递给我们。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">opt = optim.Adam(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">loss_func = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    running_corrects = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> tqdm.tqdm(train_dl):</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">        preds = model(x)</span><br><span class="line">        loss = loss_func(y, preds)</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.data[<span class="number">0</span>] * x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    epoch_loss = running_loss / len(trn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate the validation loss for this epoch</span></span><br><span class="line">    val_loss = <span class="number">0.0</span></span><br><span class="line">    model.eval() <span class="comment"># turn on evaluation mode</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> valid_dl:</span><br><span class="line">        preds = model(x)</span><br><span class="line">        loss = loss_func(y, preds)</span><br><span class="line">        val_loss += loss.data[<span class="number">0</span>] * x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    val_loss /= len(vld)</span><br><span class="line">    print(<span class="string">'Epoch: &#123;&#125;, '</span></span><br><span class="line">          <span class="string">'Training Loss: &#123;:.4f&#125;, '</span></span><br><span class="line">          <span class="string">'Validation Loss: &#123;:.4f&#125;'</span>.format(epoch, epoch_loss, val_loss))</span><br></pre></td></tr></table></figure>
<p>这里没有什么要解释的：这只是一个标准的训练循环。现在，让我们生成我们的预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test_preds = []</span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> tqdm.tqdm(test_dl):</span><br><span class="line">    preds = model(x)</span><br><span class="line">    preds = preds.data.numpy()</span><br><span class="line">    <span class="comment"># the actual outputs of the model are logits, so we need to pass these values to the sigmoid function</span></span><br><span class="line">    preds = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-preds))</span><br><span class="line">    test_preds.append(preds)</span><br><span class="line">    test_preds = np.hstack(test_preds)</span><br></pre></td></tr></table></figure>
<p>最后，我们可以将预测写入 csv 文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">"data/test.csv"</span>)</span><br><span class="line"><span class="keyword">for</span> i, col <span class="keyword">in</span> enumerate([<span class="string">"toxic"</span>, <span class="string">"severe_toxic"</span>, <span class="string">"obscene"</span>, <span class="string">"threat"</span>, <span class="string">"insult"</span>, <span class="string">"identity_hate"</span>]):</span><br><span class="line">    df[col] = test_preds[:, i]</span><br><span class="line"></span><br><span class="line">df.drop(<span class="string">"comment_text"</span>, axis=<span class="number">1</span>).to_csv(<span class="string">"submission.csv"</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>我们完成了！我们可以将该文件提交给 Kaggle，尝试完善我们的模型，更改分词器或其他类似方法，并且只需要对上面的代码进行一些更改即可。</p>
<h1 id="7-nbsp-nbsp-nbsp-nbsp-结论和进一步阅读"><a href="#7-nbsp-nbsp-nbsp-nbsp-结论和进一步阅读" class="headerlink" title="7&nbsp;&nbsp;&nbsp;&nbsp;结论和进一步阅读"></a>7&nbsp;&nbsp;&nbsp;&nbsp;结论和进一步阅读</h1><p>我希望本教程能够深入了解 torchtext 的使用方式以及其用途。尽管该库仍然很新，并且存在许多粗糙的地方，但我相信 torchtext 是朝着标准化文本预处理迈出的重要第一步，它将提高全世界 NLP 工作人员的工作效率。 如果您想查看 torchtext 用于语言建模的信息，我已上传了<a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">另一个教程</a>，其中详细介绍了语言建模和 BPTT 迭代器。如果您还有其他问题，请随时在评论中问我！</p>
<blockquote id="fn_1">
<sup>1</sup>. 下一个版本的 torchtext（以及 GitHub 上的当前版本）将能够使用字典，将名称按列映射到其对应的字段而不是列表。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. 该 API 是用于 tsv 格式的 <code>TabularDataset</code> API 的子集，因此将来可能会被废弃。<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/数据预处理-Data-Preprocessing/" rel="tag"># 数据预处理 (Data Preprocessing)</a>
              <a href="/tags/torchtext/" rel="tag"># torchtext</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/04/15/nlp-tools-a-comprehensive-tutorial-to-torchtext/" rel="prev" title="【译】torchtext 概览（实用 torchtext 第一部分）">
      <i class="fa fa-chevron-left"></i> 【译】torchtext 概览（实用 torchtext 第一部分）
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/04/17/nlp-tools-language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" rel="next" title="【译】使用 torchtext 进行语言建模（实用 torchtext 第二部分）">
      【译】使用 torchtext 进行语言建模（实用 torchtext 第二部分） <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#2-nbsp-nbsp-nbsp-nbsp-声明字段"><span class="nav-text">2    声明字段</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-nbsp-nbsp-nbsp-nbsp-构造数据集"><span class="nav-text">3    构造数据集</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-nbsp-nbsp-nbsp-nbsp-构造迭代器"><span class="nav-text">4    构造迭代器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-nbsp-nbsp-nbsp-nbsp-包装迭代器"><span class="nav-text">5    包装迭代器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-nbsp-nbsp-nbsp-nbsp-训练模型"><span class="nav-text">6    训练模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-nbsp-nbsp-nbsp-nbsp-结论和进一步阅读"><span class="nav-text">7    结论和进一步阅读</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Han Qi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">275</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">76</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Han Qi</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.2
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
